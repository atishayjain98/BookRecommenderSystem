# -*- coding: utf-8 -*-
"""RecommendationSystem.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/10K54i-b8Dc1Afgw6k-DJn476aFs3S1-n
"""

# importing libraries 
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics.pairwise import cosine_similarity

# Reading the dataset

books = pd.read_csv('/content/Books.csv')
users = pd.read_csv('/content/Users.csv')
ratings = pd.read_csv('/content/Ratings.csv')

books.head()

users.head()

ratings.head()

print(books.shape) , print(users.shape) ,print(ratings.shape)

# Check for missing values in each dataframe
print("Missing values in books: \n", books.isnull().sum())
print("\nMissing values in users: \n", users.isnull().sum())
print("\nMissing values in ratings: \n", ratings.isnull().sum())

# Check data types of variables in each dataframe
print("\nData types in books: \n", books.dtypes)
print("\nData types in users: \n", users.dtypes)
print("\nData types in ratings: \n", ratings.dtypes)

# Check for duplicates in each dataframe
print("\nDuplicates in books : \n", books.duplicated().sum())
print("\nDuplicates in users : \n", users.duplicated().sum())
print("\nDuplicates in ratings : \n", ratings.duplicated().sum())

# Check distribution of ratings
print("\nDescription of ratings: \n", ratings['Book-Rating'].describe())

# Bar chart of most popular books (top 10)
top_books = ratings['ISBN'].value_counts().head(10)
top_books.plot(kind='bar')
plt.xlabel("ISBN")
plt.ylabel("Number of ratings")
plt.show()

# Box plot of age distribution of users
users.boxplot(column='Age')
plt.show()

# Correlation matrix of all numerical variables in the dataframes
corr_matrix = pd.concat([books, users, ratings], axis=1).corr()
print(corr_matrix)

# Heatmap of the correlation matrix
sns.heatmap(corr_matrix, annot=True)
plt.show()

# Count plot of book ratings
sns.countplot(x='Book-Rating', data=ratings)
plt.xlabel("Rating")
plt.ylabel("Count")
plt.show()

# Pie chart of distribution of users by age range
age_groups = users.groupby(pd.cut(users['Age'], np.arange(0, 110, 10))).size()
age_groups.plot.pie(autopct='%1.1f%%')
plt.show()

# Histogram of distribution of number of ratings per user
num_ratings_per_user = ratings['User-ID'].value_counts()
num_ratings_per_user.plot(kind='hist', bins=10)
plt.xlabel("Number of ratings per user")
plt.ylabel("Count")
plt.show()

# Distribution of ratings for a specific book (use ISBN)
specific_book_ratings = ratings[ratings['ISBN'] == '034545104X']['Book-Rating']
specific_book_ratings.plot(kind='hist', bins=5)
plt.xlabel("Rating")
plt.ylabel("Count")
plt.show()

"""## Popularity based Recommendar System"""

# Merging the books and rating col on ISBN
book_ratings = books.merge(ratings, on = 'ISBN')
book_ratings.head()

num_rating_books = book_ratings.groupby('Book-Title')['Book-Rating'].count().reset_index().rename(columns = {'Book-Rating' : 'Num-Ratings'})
num_rating_books

avg_book_ratings = book_ratings.groupby('Book-Title')['Book-Rating'].mean().reset_index().rename(columns = {'Book-Rating' : 'Avg-Rating'})
avg_book_ratings.head()

popularity_df = num_rating_books.merge(avg_book_ratings, on='Book-Title')
popularity_df.head()

# Finding the Top 50 popular books 
pop_books = popularity_df[popularity_df['Num-Ratings'] > 200].sort_values(by = 'Avg-Rating', ascending = False).reset_index(drop = True).head(50)
pop_books

popular_books = pop_books.merge(books, on = 'Book-Title').drop_duplicates('Book-Title').reset_index(drop = True)[['Book-Title','Book-Author','Image-URL-M','Num-Ratings','Avg-Rating']]
popular_books.head()

"""## Colaborative Filtering """

x = book_ratings.groupby('User-ID').count()['Book-Rating'] > 200
freq_readers = x[x].index

filtered_ratings = book_ratings[book_ratings['User-ID'].isin(freq_readers)]

y = filtered_ratings.groupby('Book-Title').count()['Book-Rating'] >= 50
famous_books = y[y].index

final_ratings = filtered_ratings[filtered_ratings['Book-Title'].isin(famous_books)]
final_ratings

# Checking for duplicates and dropping if any
final_ratings.drop_duplicates()

pivot_t = final_ratings.pivot_table(index = 'Book-Title', columns = 'User-ID', values = 'Book-Rating')
pivot_t

pivot_t.fillna(0, inplace = True)
pivot_t

similarity_score = cosine_similarity(pivot_t)

similarity_score.shape

# to find the index of the book from the pivot table 
np.where(pivot_t.index == '1984')[0][0]

def recommend_books(book_name):
  #fetch index
  index = np.where(pivot_t.index == book_name)[0][0]
  similar_items = sorted(list(enumerate(similarity_score[index])),key = lambda x : x[1], reverse = True )[1:6]
  
  data = []
  for i in similar_items:
    item = []
    temp_df = books[books['Book-Title'] == pivot_t.index[i[0]]]
    item.extend(list(temp_df.drop_duplicates('Book-Title')['Book-Title'].values))
    item.extend(list(temp_df.drop_duplicates('Book-Title')['Book-Author'].values))
    item.extend(list(temp_df.drop_duplicates('Book-Title')['Image-URL-M'].values))

    data.append(item)

  return data

recommend_books('Message in a Bottle')

recommend_books('The Notebook')

recommend_books('The Da Vinci Code')

import pickle
pickle.dump(popular_books,open('popular.pkl','wb'))

pickle.dump(pivot_t,open('pivot_t.pkl','wb'))
pickle.dump(books,open('books.pkl','wb'))
pickle.dump(similarity_score,open('ss.pkl','wb'))

